{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhonesReviewsAspectSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTnGS-JyP9rI"
      },
      "outputs": [],
      "source": [
        "# Use amazon api for getting products and reviews\n",
        "import requests, json, math, os\n",
        "\n",
        "PRODUCT_SEARCH_URL = \"https://amazon-product-reviews-keywords.p.rapidapi.com/product/search\"\n",
        "PRODUCT_REVIEWS_URL = \"https://amazon-product-reviews-keywords.p.rapidapi.com/product/reviews\"\n",
        "API_HOST = \"amazon-product-reviews-keywords.p.rapidapi.com\"\n",
        "API_KEY = \"80a2576396msh3e6de04d43574c0p126d25jsn67dc9e6c1fd7\"\n",
        "\n",
        "AD_PREFIX = \"Sponsored Ad\"\n",
        "BRAND_REVIEWS_TARGET = 10000\n",
        "\n",
        "def get_products_page(brand, page):\n",
        "  params = {\n",
        "      \"keyword\":brand,\n",
        "      \"country\":\"GB\",\n",
        "      \"category\":\"aps\",\n",
        "      \"page\": page}\n",
        "  headers = {\n",
        "      'x-rapidapi-host': API_HOST,\n",
        "      'x-rapidapi-key': API_KEY}\n",
        "  response = requests.request(\"GET\", PRODUCT_SEARCH_URL, headers=headers, params=params)\n",
        "  return json.loads(response.text)\n",
        "\n",
        "def filter_out_ads_and_parse_products(page, brand, index):\n",
        "  products = list()\n",
        "  for product in page[\"products\"]:\n",
        "    # filter out ads because these are not the products we are looking for\n",
        "    if not product[\"title\"].startswith(AD_PREFIX):\n",
        "      products.append({\n",
        "          \"asin\": product[\"asin\"],\n",
        "          \"name\": product[\"title\"]})\n",
        "  return products\n",
        "\n",
        "def get_brand_to_products():\n",
        "  brands = [\"Apple Iphone\", \"Samsung Galaxy\", \"Google pixel\", \"Oppo\", \"Huawei Smartphone\"]\n",
        "  brand_to_products = dict.fromkeys(brands, list())\n",
        "\n",
        "  for brand in brands:\n",
        "    page_index = 1\n",
        "    while True:\n",
        "      current_page = []\n",
        "      current_page_products = []\n",
        "      current_page = get_products_page(brand, page_index)\n",
        "      page_has_products = \"products\" in current_page\n",
        "      if not page_has_products:\n",
        "        break\n",
        "      current_page_products = filter_out_ads_and_parse_products(current_page, brand, page_index)\n",
        "      \n",
        "      existing_products = brand_to_products[brand]\n",
        "      existing_and_new_products = existing_products + current_page_products if len(existing_products) > 0 else current_page_products\n",
        "      brand_to_products[brand] = existing_and_new_products\n",
        "\n",
        "      page_index += 1\n",
        "\n",
        "  return brand_to_products\n",
        "\n",
        "def get_product_reviews(asin, page):\n",
        "  params = {\n",
        "      \"asin\": asin,\n",
        "      \"page\": page,\n",
        "      \"country\": \"GB\",\n",
        "      \"variants\": \"1\"}\n",
        "  headers = {\n",
        "      'x-rapidapi-host': API_HOST,\n",
        "      'x-rapidapi-key': API_KEY}\n",
        "  response = requests.request(\"GET\", PRODUCT_REVIEWS_URL, headers=headers, params=params)\n",
        "  return json.loads(response.text)\n",
        "\n",
        "def parse_reviews(page, asin, name, index, brand):\n",
        "  reviews = []\n",
        "  for review in page[\"reviews\"]:\n",
        "    reviews.append({\n",
        "        \"review\": review[\"review\"],\n",
        "        \"id\": review[\"id\"],\n",
        "        \"title\": review[\"title\"],\n",
        "        \"rating\": review[\"rating\"],\n",
        "        \"product_asin\": asin,\n",
        "        \"product_name\": name})\n",
        "  return reviews\n",
        "\n",
        "\n",
        "def get_reviews(brand_to_products):\n",
        "  reviews = []\n",
        "  for brand in brand_to_products:\n",
        "    products = brand_to_products[brand]\n",
        "    reviews_found_for_brand = 0\n",
        "\n",
        "    for product in products:\n",
        "      page_index = 1\n",
        "      while reviews_found_for_brand < BRAND_REVIEWS_TARGET:\n",
        "        current_review_page = get_product_reviews(product[\"asin\"], page_index)\n",
        "        if \"reviews\" not in current_review_page or len(current_review_page[\"reviews\"]) == 0:\n",
        "          break\n",
        "        current_reviews = parse_reviews(current_review_page, product[\"asin\"], product[\"name\"], page_index, brand)\n",
        "        reviews.extend(current_reviews)\n",
        "\n",
        "        reviews_found_for_brand += len(current_reviews)\n",
        "        is_last_page = len(current_reviews) == 0\n",
        "        page_index +=1 \n",
        "\n",
        "  return reviews\n",
        "\n",
        "brand_to_products = get_brand_to_products()\n",
        "reviews = get_reviews(brand_to_products)\n",
        "\n",
        "for review in reviews:\n",
        "  reviews_path = '/content/drive/MyDrive/reviews'\n",
        "  file_name = \"%s.txt\" %review[\"id\"]\n",
        "  complete_path_name = os.path.join(reviews_path, file_name)\n",
        "\n",
        "  file_content = \"%s\\n%s\\n%s\\n%s\\n%s\\n\" %(review[\"product_name\"], review[\"product_asin\"], review[\"rating\"], review[\"title\"], review[\"review\"])\n",
        "  review_file = open(complete_path_name, \"w\")\n",
        "  review_file.write(file_content)\n",
        "  review_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The first idea was to only use reviews that are comparing two phones\n",
        "# but eventually all the reviews have been used\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy validate\n",
        "\n",
        "import shutil, spacy, os, random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "def should_include_review(entities):\n",
        "  product_entities = list(filter(lambda e: e.label_ == \"PRODUCT\", doc.ents))\n",
        "  unique_product_entites = set()\n",
        "  for product in product_entities:\n",
        "    unique_product_entites.add(product.text)\n",
        "  if len(unique_product_entites) >= 2:\n",
        "    return True\n",
        "  else:\n",
        "    return False \n",
        "\n",
        "reviews_directory = '/content/drive/MyDrive/reviews'\n",
        "chosen_reviews_directory = '/content/drive/MyDrive/chosen_reviews'\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "for filename in os.listdir(reviews_directory):\n",
        "  maybe_file = os.path.join(reviews_directory, filename)\n",
        "  if os.path.isfile(maybe_file):\n",
        "    file_content = open(maybe_file, 'r')\n",
        "    file_lines = file_content.readlines()\n",
        "    review = '%s.%s' %(file_lines[3], file_lines[4])\n",
        "    \n",
        "    doc = nlp(review)\n",
        "\n",
        "    if should_include_review(doc.ents):\n",
        "      review_to_include = os.path.join(chosen_reviews_directory, filename)\n",
        "      shutil.copy(maybe_file, review_to_include)"
      ],
      "metadata": {
        "id": "EepOjQAqL-eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the reviews by adding a space after a dot if the following character is a letter.\n",
        "# This will help the sentence tokenisation\n",
        "def preprocess(review):\n",
        "  for index in range(0, len(review)):\n",
        "    char = review[index]\n",
        "    if char == '.':\n",
        "      next_char = review[index+1]\n",
        "      if next_char.isalpha():\n",
        "        review = '%s %s' % (review[:index+1], review[index+1:])\n",
        "  return review\n",
        "\n",
        "reviews_directory = '/content/drive/MyDrive/reviews'\n",
        "preprocessed_directory = '/content/drive/MyDrive/preprocessed_reviews'\n",
        "\n",
        "for filename in os.listdir(reviews_directory):\n",
        "  maybe_file = os.path.join(reviews_directory, filename)\n",
        "  if os.path.isfile(maybe_file):\n",
        "    review_file = open(maybe_file, 'r')\n",
        "    file_lines = review_file.readlines()\n",
        "    review_file.close()\n",
        "\n",
        "    review = file_lines[4]\n",
        "    preprocessed_review = preprocess(review)\n",
        "\n",
        "    file_content = \"%s%s%s%s%s\" %(file_lines[0], file_lines[1], file_lines[2], file_lines[3], preprocessed_review)\n",
        "\n",
        "    preprocessed_review_path = os.path.join(preprocessed_directory, filename)\n",
        "    review_file = open(preprocessed_review_path, 'w')\n",
        "    review_file.write(file_content)\n",
        "    review_file.close()\n"
      ],
      "metadata": {
        "id": "SkP2lEl9clAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "j5bkmsFLB7TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell does sentiment analysis at a sentence level\n",
        "# and builds a json file for each review consisting of the sentiments found\n",
        "import torch, nltk, json\n",
        "nltk.download('punkt')\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "preprocessed_directory = '/content/drive/MyDrive/preprocessed_reviews'\n",
        "reviews_with_sentiments_directory = '/content/drive/MyDrive/sentimental_reviews'\n",
        "\n",
        "for filename in os.listdir(preprocessed_directory):\n",
        "  maybe_file = os.path.join(preprocessed_directory, filename)\n",
        "  if os.path.isfile(maybe_file):\n",
        "    review_file = open(maybe_file, 'r')\n",
        "    file_lines = review_file.readlines()\n",
        "    review_file.close()\n",
        "\n",
        "    review_json = {\n",
        "        'name': file_lines[0].replace('\\n', ''),\n",
        "        'asin': file_lines[1].replace('\\n', ''),\n",
        "        'rating': int(file_lines[2].replace('\\n', '')),\n",
        "        'title': file_lines[3].replace('\\n', ''),\n",
        "        'review': file_lines[4].replace('\\n', ''),\n",
        "        'sentiment_to_sentences': {'POSITIVE': [], 'NEUTRAL': [], 'NEGATIVE': []}\n",
        "    }\n",
        "\n",
        "    review_sentences = nltk.tokenize.sent_tokenize(review_json['review'])\n",
        "\n",
        "    for sentence in review_sentences:\n",
        "      sentiment_result = classifier(sentence)[0]\n",
        "\n",
        "      # if the certainty is less then 75% then assign a NEUTRAL sentiment\n",
        "      if sentiment_result['score'] > 0.75:\n",
        "        label = sentiment_result['label']\n",
        "      else:\n",
        "        label = 'NEUTRAL'\n",
        "\n",
        "      review_json['sentiment_to_sentences'][label].append(sentence)\n",
        "\n",
        "      review_json_path = os.path.join(reviews_with_sentiments_directory, '%s.json' %filename.split('.')[0])\n",
        "      with open(review_json_path, 'w') as review_json_file:\n",
        "        json.dump(review_json, review_json_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "UdDd5yQFnB5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, nltk\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from itertools import chain"
      ],
      "metadata": {
        "id": "YfvS3-GrGFwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell prepares the data for training the LDA model for topic extraction\n",
        "reviews = []\n",
        "preprocessed_directory = '/content/drive/MyDrive/preprocessed_reviews'\n",
        "preprocessed_files = os.listdir(preprocessed_directory)\n",
        "for filename in preprocessed_files:\n",
        "  review_file_path = os.path.join(preprocessed_directory, filename)\n",
        "  review_file = open(review_file_path, 'r')\n",
        "  file_lines = review_file.readlines()\n",
        "  review_file.close()\n",
        "  \n",
        "  review_text = file_lines[4]\n",
        "  reviews.append(review_text)\n",
        "\n",
        "data = pd.DataFrame(reviews, columns=['reviews'])\n",
        "data['sentences'] = data['reviews'].apply(sent_tokenize)\n",
        "data['tokens_sentences'] = data['sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
        "data['tokens_with_pos'] = data['tokens_sentences'].apply(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
        "\n",
        "# Only get nouns and adjectives for topic extraction\n",
        "def get_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# Lemmatise token with POS\n",
        "data['lemmatised_tokens'] = data['tokens_with_pos'].apply(\n",
        "    lambda tokens_with_pos: [[lemmatizer.lemmatize(el[0], get_pos(el[1])) if get_pos(el[1]) != '' else el[0] for el in token_pos] for token_pos in tokens_with_pos])\n",
        "\n",
        "#Remove stopwords\n",
        "custom_stopwords = ['can', 'come', 'get', 'go', 'know', 'like', 'make', 'may', 'need', 'say', 'see', 'take', 'use', 'want', 'would',\n",
        "                    'good', 'really', 'new', 'bad', 'even', 'also', 'well', 'great', 'excellent', \n",
        "                    'fantastic', 'happy', 'however', 'easy', 'nice', 'perfect', 'love', 'samsung',\n",
        "                    'one', 'also', 'ok', 'amazing', 'brilliant', 'best', 'smart', 'awesome', 'liked', 'pleased']\n",
        "all_stopwords = stopwords.words('english') + custom_stopwords\n",
        "data['tokens'] = data['lemmatised_tokens'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
        "data['tokens'] = data['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() and token.lower() not in all_stopwords and len(token)>1])"
      ],
      "metadata": {
        "id": "jyxN-H_vQDdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the LDA model for topic extraction\n",
        "from gensim.models import Phrases\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "import numpy as np\n",
        "\n",
        "tokens = data['tokens'].tolist()\n",
        "bigram_model = Phrases(tokens)\n",
        "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
        "tokens = list(trigram_model[bigram_model[tokens]])\n",
        "\n",
        "\n",
        "dictionary_LDA = corpora.Dictionary(tokens)\n",
        "dictionary_LDA.filter_extremes(no_below=3)\n",
        "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]\n",
        "\n",
        "\n",
        "np.random.seed(123456)\n",
        "num_topics = 10\n",
        "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary_LDA, passes=4, alpha=[0.01]*num_topics, eta=[0.01]*len(dictionary_LDA.keys()))\n",
        "\n",
        "#Display the 10 most frequent topics\n",
        "for index, topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
        "  print('%s: %s' %(str(index), topic))"
      ],
      "metadata": {
        "id": "-xP9FySPQK12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell initialises the asins final data\n",
        "import os, json\n",
        "reviews_with_sentiments_directory = '/content/drive/MyDrive/sentimental_reviews'\n",
        "\n",
        "# Map the found topics to aspects\n",
        "topic_id_to_aspect = {\n",
        "    0: 'overall',\n",
        "    1: 'battery',\n",
        "    2: 'camera',\n",
        "    3: 'overall',\n",
        "    4: 'value for price',\n",
        "    5: 'service & warranty',\n",
        "    6: 'shopping experience / delivery',\n",
        "    7: 'screen',\n",
        "    8: 'brand & os',\n",
        "    9: 'screen'}\n",
        "\n",
        "def init_aspect_to_sentiments():\n",
        "  return {\n",
        "      'overall': {'POSITIVE': 0, 'NEUTRAL': 0, 'NEGATIVE': 0},\n",
        "      'battery': {'POSITIVE': 0, 'NEUTRAL': 0, 'NEGATIVE': 0},\n",
        "      'camera': {'POSITIVE': 0, 'NEUTRAL': 0, 'NEGATIVE': 0},\n",
        "      'screen': {'POSITIVE': 0, 'NEUTRAL': 0, 'NEGATIVE': 0},\n",
        "      'value for price': {'POSITIVE': 0, 'NEUTRAL': 0, 'NEGATIVE': 0},\n",
        "      'service & warranty': {'POSITIVE': 0, 'NEUTRAL': 0, 'NEGATIVE': 0},\n",
        "      'brand & os': {'POSITIVE': 0, 'NEUTRAL': 0, 'NEGATIVE': 0},\n",
        "      'shopping experience / delivery': {'POSITIVE': 0, 'NEUTRAL': 0, 'NEGATIVE': 0}}\n",
        "\n",
        "def get_all_asins():\n",
        "  asins = set()\n",
        "  \n",
        "  reviews_with_sentiments = os.listdir(reviews_with_sentiments_directory)\n",
        "  for filename in reviews_with_sentiments:\n",
        "    review_file_path = os.path.join(reviews_with_sentiments_directory, filename)\n",
        "    review_file = open(review_file_path, 'r')\n",
        "    review_dict = json.load(review_file)\n",
        "    review_file.close()\n",
        "    asin = review_dict['asin']\n",
        "    asins.add(asin)\n",
        "\n",
        "  return asins\n",
        "\n",
        "asin_to_data = {}\n",
        "asins = get_all_asins()\n",
        "for asin in asins:\n",
        "  asin_to_data[asin] = {\n",
        "      'asin': asin,\n",
        "      'name': '',\n",
        "      'ratings': [],\n",
        "      'aspect_to_sentiments': init_aspect_to_sentiments()}\n"
      ],
      "metadata": {
        "id": "4llTMR4TWkZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell build the phones final data by taking the sentences with sentiments\n",
        "# and figuring out what is the aspect described in them\n",
        "import json, os\n",
        "\n",
        "reviews_with_sentiments_directory = '/content/drive/MyDrive/sentimental_reviews'\n",
        "\n",
        "def build_aspect_to_sentiments(sentiment_to_sentences):\n",
        "  aspect_to_sentiments = init_aspect_to_sentiments()\n",
        "  for sentiment in sentiment_to_sentences:\n",
        "    sentences = sentiment_to_sentences[sentiment]\n",
        "    for sentence in sentences:\n",
        "      tokens = word_tokenize(sentence)\n",
        "      topics = lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20)\n",
        "      topic_id = sorted(lda_model[dictionary_LDA.doc2bow(tokens)], key=lambda result: result[1], reverse=True)[0][0]\n",
        "      aspect = topic_id_to_aspect[topic_id]\n",
        "\n",
        "      aspect_to_sentiments[aspect][sentiment] += 1\n",
        "\n",
        "  return aspect_to_sentiments\n",
        "\n",
        "reviews_with_sentiments_files = os.listdir(reviews_with_sentiments_directory)\n",
        "for filename in reviews_with_sentiments_files:\n",
        "  review_file_path = os.path.join(reviews_with_sentiments_directory, filename)\n",
        "  review_file = open(review_file_path, 'r')\n",
        "  review_dict = json.load(review_file)\n",
        "  review_file.close()\n",
        "\n",
        "  review_aspect_to_sentiments = build_aspect_to_sentiments(review_dict['sentiment_to_sentences'])\n",
        "  asin = review_dict['asin']\n",
        "\n",
        "  asin_to_data[asin]['ratings'].append(int(review_dict['rating']))\n",
        "  asin_to_data[asin]['name'] = review_dict['name']\n",
        "\n",
        "  for aspect in review_aspect_to_sentiments:\n",
        "    aspect_sentiments = review_aspect_to_sentiments[aspect]\n",
        "    for sentiment in aspect_sentiments:\n",
        "      asin_to_data[asin]['aspect_to_sentiments'][aspect][sentiment] += aspect_sentiments[sentiment]\n",
        "\n",
        "# Finally store the data into json files that are going to be read by the web platform\n",
        "asin_data_directory = '/content/drive/MyDrive/asin_data'\n",
        "for asin in asin_to_data:\n",
        "  ratings = asin_to_data[asin]['ratings']\n",
        "  asin_to_data[asin]['rating'] = round(sum(ratings) / len(ratings), 2)\n",
        "\n",
        "  asin_data_json_path = os.path.join(asin_data_directory, '%s.json' %asin)\n",
        "  with open(asin_data_json_path, 'w') as asin_data_json_file:\n",
        "    json.dump(asin_to_data[asin], asin_data_json_file)\n"
      ],
      "metadata": {
        "id": "nrC05y9qt8Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwk5kKj5qaQ2",
        "outputId": "f8c2d6f8-4dfe-49bd-d0fa-f3d543a9bc86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}